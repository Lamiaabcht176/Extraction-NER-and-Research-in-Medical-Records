{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity extraction is a process in natural language processing (NLP) that identifies and categorizes specific elements within text, such as names of people, organizations, locations, dates, quantities, and other relevant entities. By analyzing unstructured text, named entity extraction helps transform raw data into structured information, allowing systems to better understand and organize text content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will address the task of recognizing entities in French biomedical texts from [The QUAERO French Medical Corpus](https://quaerofrenchmed.limsi.fr/), a dataset designed for Named Entity Recognition (NER) in the biomedical domain. his corpus consists of manually annotated MEDLINE titles and EMEA documents, with entity annotations based on concepts from the Unified Medical Language System [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/).  \n",
    "The annotations provide a standardized set of biomedical entities, making the dataset an essential resource for extracting and classifying medical terms in French text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus contains annotations for ten types of clinical entities, each labeled according to specific categories: Anatomy (ANAT), Chemical and Drugs (CHEM), Devices (DEVI), Disorders (DISO), Geographic Areas (GEOG), Living Beings (LIVB), Objects (OBJC), Phenomena (PHEN), Physiology (PHYS), and Procedures (PROC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will only use the MEDLINE texts. [MEDLINE](https://www.nlm.nih.gov/bsd/medline.html) is the U.S. National Library of Medicine® (NLM) premier bibliographic database that contains more than 25 million references to journal articles in life sciences with a concentration on biomedicine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show a sample annotation for a MEDLINE text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample MEDLINE title 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *Chirurgie de la communication interauriculaire du type \" sinus venosus \" .*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample MEDLINE title 1 annotations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T1           PROC 0 9             Chirurgie\n",
    "\n",
    "\n",
    "T2           DISO 16 46          communication interauriculaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the text between characters 0 and 9 is assigned a label **PROC** (= procedure). The token which corresponds to this text is “**Chirurgie**”. \n",
    "Second annotation is for the text between characters 16 and 46 (which covers tokens “**communication interauriculaire**”) and is assigned label **DISO** (= disorder).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we are interested to train a classifier able to extract those text segments and identify them with the correct label. We will use a class of statistical modeling method used for structured prediction known as Conditional Random Fields (CRFs), which falls into the sequence modeling family. Whereas a discrete classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account. They are used to encode known relationships between observations and construct consistent interpretations and are often used for labeling or parsing of sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains three subdirectories: train, test and dev. For this notebook, we will use only the first one. It contains 1670 files, including 4 files about configuration and statistics. The rest of the files is divided in two types: .TXT files which contain the text of the sentences and annotations files (.ann) with information about the text segments, its types, etc., as we explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be preprocessing input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data train set \n",
    "path_train = \"C:\\\\Users\\\\lamia\\\\Desktop\\\\Extraction-NER-Recherche\\\\Analyses\\\\train\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"L' OMS planifie pour l' Europe l' application du processus des soins infirmiers . Compte rendu de la session du groupe technique d' experts en soins infirmiers et obstétricaux du Bureau régional de l' Europe de l' OMS , Nottingham , 14 - 17 décembre 1976\\n\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To read a file and obtain its content\n",
    "data = NER.read_file(path_train,\"14448\",\".txt\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T1\\tGEOG 24 30\\tEurope\\n',\n",
      " '#1\\tAnnotatorNotes T1\\tC0015176\\n',\n",
      " 'T2\\tPHEN 49 58\\tprocessus\\n',\n",
      " '#2\\tAnnotatorNotes T2\\tC1522240\\n',\n",
      " 'T3\\tPROC 63 79\\tsoins infirmiers\\n',\n",
      " '#3\\tAnnotatorNotes T3\\tC0028682\\n',\n",
      " 'T4\\tLIVB 69 79\\tinfirmiers\\n',\n",
      " '#4\\tAnnotatorNotes T4\\tC0028676\\n',\n",
      " 'T5\\tPROC 143 159\\tsoins infirmiers\\n',\n",
      " '#5\\tAnnotatorNotes T5\\tC0028682\\n',\n",
      " 'T6\\tLIVB 149 159\\tinfirmiers\\n',\n",
      " '#6\\tAnnotatorNotes T6\\tC0028676\\n',\n",
      " 'T7\\tGEOG 201 207\\tEurope\\n',\n",
      " '#7\\tAnnotatorNotes T7\\tC0015176\\n']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "data = NER.read_file(path_train,\"14448\",\".ann\")\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the first file read was \"14448.txt\" while the second one was \"14448.ann\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'label': ['GEOG', '24', '30'], 'text': 'Europe'},\n",
      " 'T2': {'label': ['PHEN', '49', '58'], 'text': 'processus'},\n",
      " 'T3': {'label': ['PROC', '63', '79'], 'text': 'soins infirmiers'},\n",
      " 'T4': {'label': ['LIVB', '69', '79'], 'text': 'infirmiers'},\n",
      " 'T5': {'label': ['PROC', '143', '159'], 'text': 'soins infirmiers'},\n",
      " 'T6': {'label': ['LIVB', '149', '159'], 'text': 'infirmiers'},\n",
      " 'T7': {'label': ['GEOG', '201', '207'], 'text': 'Europe'}}\n"
     ]
    }
   ],
   "source": [
    "d = NER.ann_text2dict(data)\n",
    "pprint.pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER.collect_files(path_train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of ann files 833\n"
     ]
    }
   ],
   "source": [
    "lnew = NER.ann_files2dict('train_ann',path_train,'train')\n",
    "\n",
    "print(\"# of ann files\",len(lnew))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one situation that we didn’t mentioned before: it is possible that more labels are assigned to the same token (annotations overlap). In this case, we will only choose one of them and discard the other. For example, let’s assume that we have the following text: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Prévalence des marqueurs des *virus des hépatites A* , B , C à La Réunion ( Hôpital sud et prison de Saint Pierre ).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following annotations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T1           CHEM 15 24       marqueurs\n",
    "\n",
    "T2           LIVB 29 34           virus\n",
    "\n",
    "T3           DISO 39 50          hépatites A\n",
    "\n",
    "T4           DISO 39 48;53 54             hépatites B\n",
    "\n",
    "T5           DISO 39 48;57 58             hépatites C\n",
    "\n",
    "T6           GEOG 61 71        La Réunion\n",
    "\n",
    "T7           LIVB 29 48;57 58              virus des hépatites C\n",
    "\n",
    "T8           LIVB 29 48;53 54              virus des hépatites B\n",
    "\n",
    "T9           LIVB 29 50           virus des hépatites A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that:\n",
    "* annotation T2 identifies the word 'virues' (characters 29-34) as a Living Being (LIVB),\n",
    "* annotation T9 identifies the segment 'virus des hépatites A' (characters 29-50) as a Living Being (LIVB),\n",
    "* annotation T8 identifies the segment 'virus des hépatites B' (characters 29-48 and 53-54) as a Living Being (LIVB), and\n",
    "* annotation T7 identifies the segment 'virus des hépatites C' (characters 29-48 and 57-58) as a Living Being (LIVB)\n",
    "\n",
    "In those cases, we will discard the annotation T2 which is included into the others and keep T7, T8 and T9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with the annotation dictionary previously obtained d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'label': ['GEOG', '24', '30'], 'text': 'Europe'},\n",
      " 'T2': {'label': ['PHEN', '49', '58'], 'text': 'processus'},\n",
      " 'T3': {'label': ['PROC', '63', '79'], 'text': 'soins infirmiers'},\n",
      " 'T4': {'label': ['LIVB', '69', '79'], 'text': 'infirmiers'},\n",
      " 'T5': {'label': ['PROC', '143', '159'], 'text': 'soins infirmiers'},\n",
      " 'T6': {'label': ['LIVB', '149', '159'], 'text': 'infirmiers'},\n",
      " 'T7': {'label': ['GEOG', '201', '207'], 'text': 'Europe'}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'label': ['GEOG', '24', '30'], 'text': 'Europe'},\n",
      " 'T2': {'label': ['PHEN', '49', '58'], 'text': 'processus'},\n",
      " 'T3': {'label': ['PROC', '63', '79'], 'text': 'soins infirmiers'},\n",
      " 'T5': {'label': ['PROC', '143', '159'], 'text': 'soins infirmiers'},\n",
      " 'T7': {'label': ['GEOG', '201', '207'], 'text': 'Europe'}}\n"
     ]
    }
   ],
   "source": [
    "d1 = NER.remove_contained(d)\n",
    "pprint.pprint(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that segments T4 and T6 were removed because T4 was contained into T3 and T6 was contained into T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set train\n",
      "Number of non continuous segments 13 % 0.43420173680694724\n",
      "Total number of segments 2994\n"
     ]
    }
   ],
   "source": [
    "NER.count_non_continuous('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that the number of non-contiguous segments is very low and that is why we decided to ignore them for this version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information that we need to use to train the classifier is contained in two independent structures: the .TXT files and the annotation dictionaries. Let's now combine and simplfy them.\n",
    "\n",
    "The function \"simple_dic\" will be used to simplify the dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'GEOG', 'range': ['24', '30'], 'text': 'Europe'},\n",
      " {'label': 'PHEN', 'range': ['49', '58'], 'text': 'processus'},\n",
      " {'label': 'PROC', 'range': ['63', '79'], 'text': 'soins infirmiers'},\n",
      " {'label': 'PROC', 'range': ['143', '159'], 'text': 'soins infirmiers'},\n",
      " {'label': 'GEOG', 'range': ['201', '207'], 'text': 'Europe'}]\n"
     ]
    }
   ],
   "source": [
    "sdic = NER.simple_dic(d1)\n",
    "pprint.pprint(sdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ann_dic': [{'label': 'PROC', 'range': ['0', '10'], 'text': 'Traitement'},\n",
      "             {'label': 'DISO',\n",
      "              'range': ['15', '36'],\n",
      "              'text': 'métastases hépatiques'},\n",
      "             {'label': 'DISO',\n",
      "              'range': ['41', '60'],\n",
      "              'text': 'cancers colorectaux'}],\n",
      " 'txt': ['Traitement des métastases hépatiques des cancers colorectaux : '\n",
      "         \"jusqu' où aller ?\\n\"]}\n"
     ]
    }
   ],
   "source": [
    "set = 'train'\n",
    "NER.mix_txt_ann('train_txt',path_train,set)\n",
    "lista = NER.load_pickle(set+'_txt_ann')\n",
    "pprint.pprint(lista[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify more this structure converting the list of dictionaries that corresponds to the annotation part in a list of tuples. We need also to tag all segments included in the TXT segments. We already have some of them tagged, but others don't. We will tag them as 'NONE' indicating that this tag is none of the others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ann_dic': [(0, 10, 'PROC', 'Traitement'),\n",
      "             (11, 14, 'NONE', 'des'),\n",
      "             (15, 36, 'DISO', 'métastases hépatiques'),\n",
      "             (37, 40, 'NONE', 'des'),\n",
      "             (41, 60, 'DISO', 'cancers colorectaux'),\n",
      "             (61, 80, 'NONE', \": jusqu' où aller ?\")],\n",
      " 'txt': \"Traitement des métastases hépatiques des cancers colorectaux : jusqu' \"\n",
      "        'où aller ?\\n'}\n"
     ]
    }
   ],
   "source": [
    "set = 'train'\n",
    "NER.complete_segments(set)\n",
    "new1 = NER.load_pickle(set + '_txt_ann2')\n",
    "pprint.pprint(new1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of function \"ldic2ltok_lab\", we will tokenize each one of the text segments and tag each token with the corresponding tag i.e. the segment tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_tok_lab = NER.ldic2ltok_lab(new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER.save_pickle(ls_tok_lab,set + '_txt_ann3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier est un PDF natif.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemple d'utilisation\n",
    "file_path = \"C:/Users/lamia/Desktop/Extraction-NER-Recherche/P0001/PDF/analyse.pdf\"\n",
    "if NER.is_pdf(file_path):\n",
    "    print(\"Le fichier est un PDF natif.\")\n",
    "else:\n",
    "    print(\"Le fichier n'est pas un PDF natif.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "pdf_path = \"C:/Users/lamia/Desktop/Extraction-NER-Recherche/P0001/PDF/FichePatient.pdf\"\n",
    "if NER.is_scanned_pdf(pdf_path):\n",
    "    print(\"The PDF seems to contain scanned images.\")\n",
    "else:\n",
    "    print(\"The PDF does not seem to contain scanned images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
